import numpy as np
import pandas as pd
from ampligraph.datasets import load_from_csv
from ampligraph.latent_features import ScoringBasedEmbeddingModel
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from ampligraph.evaluation import mrr_score, hits_at_n_score
from ampligraph.compat import evaluate_performance


# ——— Configuration ———
TRAIN_FILE = "train.tsv"
VALID_FILE = "valid.tsv"
TEST_FILE = "test.tsv"

EPOCHS      = 200
BATCHES     = 150  # number of batches per epoch

PCA_DIM = 20
K_MIN, K_MAX = 3, 10
OUTPUT_CSV = "author_clusters_ag2.csv"

# ——— Load splits ———
X_train_raw = load_from_csv(".", TRAIN_FILE, sep="\t")
X_valid_raw = load_from_csv(".", VALID_FILE, sep="\t")
X_test_raw  = load_from_csv(".", TEST_FILE,  sep="\t")

# —– Build vocab from train —–
all_ents = np.concatenate([
    X_train_raw[:, 0].astype(str),
    X_train_raw[:, 2].astype(str)
])
mask = all_ents != 'nan'
ents = np.unique(all_ents[mask])
rels = np.unique(X_train_raw[:, 1])
ent_to_idx = {e: i for i, e in enumerate(ents)}
rel_to_idx = {r: i for i, r in enumerate(rels)}

# —– Helper to map and filter triples —–
def map_and_filter(X_raw, ent_to_idx, rel_to_idx):
    mapped = []
    for h, r, t in X_raw:
        if h in ent_to_idx and r in rel_to_idx and t in ent_to_idx:
            mapped.append([ent_to_idx[h], rel_to_idx[r], ent_to_idx[t]])
    return np.array(mapped, dtype=int)

X_train = map_and_filter(X_train_raw, ent_to_idx, rel_to_idx)
X_valid = map_and_filter(X_valid_raw, ent_to_idx, rel_to_idx)
X_test  = map_and_filter(X_test_raw,  ent_to_idx, rel_to_idx)

print(f"Training ComplEx | dim=100 | negs=5")
# Instantiate model
model = ScoringBasedEmbeddingModel(
    eta=5,
    k=100,
    scoring_type="ComplEx",
    seed=0
)
model.compile(optimizer='adam', loss='pairwise')

# Fit model
batch_size = max(1, X_train.shape[0] // BATCHES)
model.fit(
    X_train,
    batch_size=batch_size,
    epochs=EPOCHS,
    verbose=False
)

# Evaluate on validation
ranks = evaluate_performance(
    X_valid,
    model=model,
    filter_triples=X_train,
    corrupt_side='s,o',
    batch_size=256
)
mrr  = mrr_score(ranks)
h10  = hits_at_n_score(ranks, 10)
print(f"  → MRR={mrr:.4f}, Hits@10={h10:.4f}")


# 3) Extract entity embeddings (shape: [n_entities, dim])
embeddings = model.get_embeddings()
print(f"Fetched embeddings array of shape {embeddings.shape}")

# 4) Identify author entities by prefix
author_ids = [ent_to_idx[e] for e in ents if e.startswith("author:")]
auth_emb = embeddings[author_ids]
print(f"Selected {len(author_ids)} author embeddings")

# 5) PCA reduction
pca = PCA(n_components=PCA_DIM)
reduced = pca.fit_transform(auth_emb)
print(f"PCA retains {pca.explained_variance_ratio_.sum():.3f} variance")

# 6) Determine best k via silhouette
best_k, best_score = None, -1
scores = {}
for k in range(K_MIN, K_MAX + 1):
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(reduced)
    score = silhouette_score(reduced, labels)
    scores[k] = score
    print(f"k={k}, silhouette={score:.3f}")
    if score > best_score:
        best_score, best_k = score, k
print(f"Optimal k: {best_k} (silhouette={best_score:.3f})")

# 7) Final clustering and save
# km = KMeans(n_clusters=best_k, random_state=42)
# final_labels = km.fit_predict(reduced)
# df_out = pd.DataFrame({
#     'author_id': author_ids,
#     'author_entity': [idx_to_ent[i] for i in author_ids],
#     'cluster': final_labels
# })
# df_out.to_csv(OUTPUT_CSV, index=False)
# print(f"Saved author clusters to {OUTPUT_CSV}")
