import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd

# ——— Parameters ———
TRAIN_FILE    = "train.tsv"  # generated by C1_prepare_dataset.py
EMBED_DIM     = 50           # embedding dimensionality
MARGIN        = 1.0          # margin for ranking loss
LEARNING_RATE = 0.01
EPOCHS        = 50
BATCH_SIZE    = 512

# URIs to use for the example queries
PAPER_URI     = "http://research.publications.com/instance#Paper_9169568"
CITES_URI     = "http://research.publications.com/ontology#cites"
WRITTENBY_URI = "http://research.publications.com/ontology#writtenBy"

# ——— Load triples and build mappings ———
df = pd.read_csv(
    TRAIN_FILE,
    sep="\t",
    names=["head", "relation", "tail"],
    dtype=str,
    keep_default_na=False,
)

# Combine heads and tails, then extract unique entity and relation lists
all_entities = pd.concat([df["head"], df["tail"]])
entities = list(all_entities.unique())
relations = list(df["relation"].unique())

ent2id = {ent: idx for idx, ent in enumerate(entities)}
rel2id = {rel: idx for idx, rel in enumerate(relations)}

# Convert triples to integer IDs
triples = torch.LongTensor([
    [ent2id[h], rel2id[r], ent2id[t]]
    for h, r, t in df.values
])

# ——— TransE model definition ———
class TransEModel(nn.Module):
    def __init__(self, num_entities, num_relations, dim):
        super().__init__()
        self.entity_embeddings = nn.Embedding(num_entities, dim)
        self.relation_embeddings = nn.Embedding(num_relations, dim)
        nn.init.xavier_uniform_(self.entity_embeddings.weight.data)
        nn.init.xavier_uniform_(self.relation_embeddings.weight.data)

    def forward(self, heads, rels, tails):
        # L2 norm of (e_h + w_r - e_t)
        h_emb = self.entity_embeddings(heads)
        r_emb = self.relation_embeddings(rels)
        t_emb = self.entity_embeddings(tails)
        return torch.norm(h_emb + r_emb - t_emb, p=2, dim=1)

# ——— Training loop ———
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = TransEModel(len(entities), len(relations), EMBED_DIM).to(device)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.MarginRankingLoss(margin=MARGIN)

# Precompute all entity IDs for negative sampling
all_entity_ids = torch.arange(len(entities), device=device)

for epoch in range(1, EPOCHS + 1):
    permutation = torch.randperm(len(triples))
    epoch_losses = []
    for i in range(0, len(permutation), BATCH_SIZE):
        batch_idx = permutation[i:i + BATCH_SIZE]
        positive = triples[batch_idx].to(device)

        # Create negative samples by corrupting the tail
        neg_tails = all_entity_ids[
            torch.randint(0, len(entities), (positive.size(0),))
        ]
        negative = torch.stack([positive[:, 0], positive[:, 1], neg_tails], dim=1)

        model.zero_grad()
        pos_scores = model(positive[:, 0], positive[:, 1], positive[:, 2])
        neg_scores = model(negative[:, 0], negative[:, 1], negative[:, 2])

        # We want pos_score + margin < neg_score -> label = -1
        target = -torch.ones_like(pos_scores, device=device)
        loss = criterion(pos_scores, neg_scores, target)
        loss.backward()
        optimizer.step()
        epoch_losses.append(loss.item())

    avg_loss = np.mean(epoch_losses)
    print(f"Epoch {epoch:2d}/{EPOCHS} – Avg Loss: {avg_loss:.4f}")

# Predicts embedding for a cited paper and for its author by relation inversion.
# Identifies the actual author in the KG whose embedding is closest.

# ——— Inference: compute embeddings and make predictions ———
model.eval()
with torch.no_grad():
    entity_matrix = model.entity_embeddings.weight.data.cpu().numpy()
    relation_matrix = model.relation_embeddings.weight.data.cpu().numpy()

# 1) Compute predicted embedding of the cited paper: e_paper + w_cites
paper_vec = entity_matrix[ent2id[PAPER_URI]]
cites_vec = relation_matrix[rel2id[CITES_URI]]
predicted_cited_vec = paper_vec + cites_vec

print("Predicted embedding for the cited paper (dimension", EMBED_DIM, "):")
print(predicted_cited_vec)

# Filter only entities that are papers
paper_ids = [ent2id[e] for e in entities if "Paper" in e or "paper" in e]
paper_embeddings = entity_matrix[paper_ids]

# Compute distances to the predicted vector
distances = np.linalg.norm(paper_embeddings - predicted_cited_vec, axis=1)

# Find the closest
best_id = paper_ids[np.argmin(distances)]
closest_paper = entities[best_id]
closest_dist = distances.min()

print(f"\nClosest paper in the KG: {closest_paper}")
print(f"Distance to the predicted vector: {closest_dist:.4f}")

# 2) Compute predicted author vector by inverting writtenBy: vec_cited - w_writtenBy
writtenby_vec = relation_matrix[rel2id[WRITTENBY_URI]]
predicted_author_vec = predicted_cited_vec - writtenby_vec

# Filter entity IDs to only those that are authors (assuming URIs contain “Author”)
author_ids = [
    ent2id[e] for e in entities
    if "Author" in e or "author" in e
]

# Compute Euclidean distances and find the closest real author
author_embeddings = entity_matrix[author_ids]
distances = np.linalg.norm(author_embeddings - predicted_author_vec, axis=1)
best_idx = author_ids[np.argmin(distances)]
closest_author = entities[best_idx]
closest_distance = distances.min()

print(f"\nClosest author match: {closest_author}")
print(f"Distance to predicted author vector: {closest_distance:.4f}")
